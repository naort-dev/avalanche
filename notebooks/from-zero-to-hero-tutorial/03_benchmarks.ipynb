{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16b4b118",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "description: Create your Continual Learning Benchmark and Start Prototyping\n",
    "---\n",
    "\n",
    "# Benchmarks\n",
    "\n",
    "Welcome to the \"_benchmarks_\" tutorial of the \"_From Zero to Hero_\" series. In this part we will present the functionalities offered by the `Benchmarks` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239f0a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install avalanche-lib==0.4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6142a7a6",
   "metadata": {},
   "source": [
    "## üéØ Nomenclature\n",
    "\n",
    "Avalanche Benchmarks provide the data that you will for training and evaluating your model. Benchmarks have the following structure:\n",
    "- A `Benchmark` is a collection of streams. Most benchmarks have at least a `train_stream` and a `test_stream`;\n",
    "- A `Stream` is a sequence of `Experience`s. It can be a list or a generator;\n",
    "- An `Experience` contains all the information available at a certain time `t`;\n",
    "- `AvalancheDataset` is a wrapper of PyTorch datasets. It provides functionalities used by the training module, such as concatenation, subsampling, and management of augmentations.\n",
    "\n",
    "### üìö The Benchmarks Module\n",
    "\n",
    "The `bechmarks` module offers:\n",
    "\n",
    "* **Datasets**: Pytorch datasets are wrapped in an `AvalancheDataset` to provide additional functionality.\n",
    "* **Classic Benchmarks**: classic benchmarks used in CL litterature ready to be used with great flexibility.\n",
    "* **Benchmarks Generators**: a set of functions you can use to create your own benchmark and streams starting from any kind of data and scenario, such as class-incremental or task-incremental streams.\n",
    "\n",
    "But let's see how we can use this module in practice!\n",
    "\n",
    "## üñºÔ∏è Datasets\n",
    "\n",
    "Let's start with the `Datasets`. When using _Avalanche_, your code will manipulate `AvalancheDataset`s. It is a wrapper compatible with pytorch and torchvision map-style datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b3c3ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\w-32\\mambaforge\\envs\\avl_dev\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: torch.Size([1, 28, 28]) - 5\n",
      "Batch 0: torch.Size([32, 1, 28, 28]) - torch.Size([32])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from avalanche.benchmarks.datasets import MNIST\n",
    "from avalanche.benchmarks.datasets.dataset_utils import default_dataset_location\n",
    "from avalanche.benchmarks.utils import make_classification_dataset\n",
    "\n",
    "# Most datasets in Avalanche are automatically downloaded the first time you use them\n",
    "# and stored in a default location. You can change this folder by calling\n",
    "# avalanche.benchmarks.utils.set_dataset_root(new_location)\n",
    "datadir = default_dataset_location('mnist')\n",
    "\n",
    "# As we would simply do with any Pytorch dataset we can create the train and \n",
    "# test sets from it. We could use any of the above imported Datasets, but let's\n",
    "# just try to use the standard MNIST.\n",
    "train_MNIST = MNIST(datadir, train=True, download=True)\n",
    "test_MNIST = MNIST(datadir, train=False, download=True)\n",
    "\n",
    "# transformations are managed by the AvalancheDataset\n",
    "train_transforms = torchvision.transforms.ToTensor()\n",
    "eval_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((32, 32))\n",
    "])\n",
    "\n",
    "# wrap datasets into Avalanche datasets\n",
    "# notice that AvalancheDatasets have multiple transform groups\n",
    "# `train` and `eval` are the default ones, but you can add more (e.g. replay-specific transforms)\n",
    "train_MNIST = make_classification_dataset(\n",
    "    train_MNIST,\n",
    "    transform_groups={\n",
    "        'train': train_transforms, \n",
    "        'eval': eval_transforms\n",
    "    }\n",
    ")\n",
    "test_MNIST = make_classification_dataset(\n",
    "    test_MNIST,\n",
    "    transform_groups={\n",
    "        'train': train_transforms, \n",
    "        'eval': eval_transforms\n",
    "    }\n",
    ")\n",
    "\n",
    "# we can iterate the examples as we would do with any Pytorch dataset\n",
    "for i, example in enumerate(train_MNIST):\n",
    "    print(f\"Sample {i}: {example[0].shape} - {example[1]}\")\n",
    "    break\n",
    "\n",
    "# or use a Pytorch DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_MNIST, batch_size=32, shuffle=True\n",
    ")\n",
    "for i, (x, y, t) in enumerate(train_loader):\n",
    "    print(f\"Batch {i}: {x.shape} - {y.shape}\")\n",
    "    break\n",
    "\n",
    "# we can also switch between train/eval transforms\n",
    "train_MNIST.train()\n",
    "print(train_MNIST[0][0].shape)\n",
    "\n",
    "train_MNIST.eval()\n",
    "print(train_MNIST[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b874c06",
   "metadata": {},
   "source": [
    "In this example we created a classification dataset. Avalanche expects an attribute `targets` for classification dataset, which is provided by MNIST and most classification datasets.\n",
    "Avalanche provides concatenation and subsampling, which also keep the dataset attributes consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bf09cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "120000\n",
      "5\n",
      "[5, 0, 4, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_MNIST))  # 60k\n",
    "print(len(train_MNIST.concat(train_MNIST)))  # 120k\n",
    "\n",
    "# subsampling is often used to create streams or replay buffers!\n",
    "dsub = train_MNIST.subset([0, 1, 2, 3, 4])\n",
    "print(len(dsub))  # 5\n",
    "# targets are preserved when subsetting\n",
    "print(list(dsub.targets))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0c2028f",
   "metadata": {},
   "source": [
    "## üèõÔ∏è Classic Benchmarks\n",
    "\n",
    "Most benchmarks will provide two streams: the `train_stream` and `test_stream`.\n",
    "Often, these are two parallel streams of the same length, where each experience is sampled from the same distribution (e.g. same set of classes). \n",
    "Some benchmarks may have a single test experience with the whole test dataset.\n",
    "\n",
    "Experiences provide all the information needed to update the model, such as the new batch of data, and they may be decorated with attributes that are helpful for training or logging purposes.\n",
    "Long streams can be generated on-the-fly to reduce memory requirements and avoiding long preprocessing time during the benchmark creation step.\n",
    "\n",
    "We will use `SplitMNIST`, a popular CL benchmark which is the class-incremental version of `MNIST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62bd72c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stream: train\n",
      "EID=0, classes=[5, 6], tasks=[0]\n",
      "data: 11339 samples\n",
      "EID=1, classes=[1, 2], tasks=[1]\n",
      "data: 12700 samples\n",
      "EID=2, classes=[0, 8], tasks=[2]\n",
      "data: 11774 samples\n",
      "EID=3, classes=[9, 3], tasks=[3]\n",
      "data: 12080 samples\n",
      "EID=4, classes=[4, 7], tasks=[4]\n",
      "data: 12107 samples\n",
      "EID=0, classes=[5, 6], task=[4]\n",
      "EID=1, classes=[1, 2], task=[4]\n",
      "EID=2, classes=[0, 8], task=[4]\n",
      "EID=3, classes=[9, 3], task=[4]\n",
      "EID=4, classes=[4, 7], task=[4]\n"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "\n",
    "bm = SplitMNIST(\n",
    "    n_experiences=5,  # 5 incremental experiences\n",
    "    return_task_id=True,  # add task labels\n",
    "    seed=1  # you can set the seed for reproducibility. This will fix the order of classes\n",
    ")\n",
    "\n",
    "# streams have a name, used for logging purposes\n",
    "# each metric will be logged with the stream name\n",
    "print(f'--- Stream: {bm.train_stream.name}')\n",
    "# each stream is an iterator of experiences\n",
    "for exp in bm.train_stream:\n",
    "    # experiences have an ID that denotes its position in the stream\n",
    "    # this is used only for logging (don't rely on it for training!)\n",
    "    eid = exp.current_experience\n",
    "    # for classification benchmarks, experiences have a list of classes in this experience\n",
    "    clss = exp.classes_in_this_experience\n",
    "    # you may also have task labels\n",
    "    tls = exp.task_labels\n",
    "    print(f\"EID={eid}, classes={clss}, tasks={tls}\")\n",
    "    # the experience provides a dataset\n",
    "    print(f\"data: {len(exp.dataset)} samples\")\n",
    "\n",
    "for exp in bm.test_stream:\n",
    "    print(f\"EID={exp.current_experience}, classes={exp.classes_in_this_experience}, task={tls}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f226374",
   "metadata": {},
   "source": [
    "## üê£ Benchmarks Generators\n",
    "\n",
    "The most basic way to create a benchmark is to use the `benchmark_from_datasets` method. It takes a list of datasets for each stream and returns a benchmark with the specified streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caf6ab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train - len 2\n",
      "test - len 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from avalanche.benchmarks.datasets.torchvision_wrapper import CIFAR10\n",
    "from avalanche.benchmarks.scenarios.dataset_scenario import benchmark_from_datasets\n",
    "\n",
    "\n",
    "datadir = default_dataset_location('mnist')\n",
    "train_MNIST = make_classification_dataset(MNIST(datadir, train=True, download=True))\n",
    "test_MNIST = make_classification_dataset(MNIST(datadir, train=False, download=True))\n",
    "\n",
    "datadir = default_dataset_location('cifar10')\n",
    "train_CIFAR10 = make_classification_dataset(CIFAR10(datadir, train=True, download=True))\n",
    "test_CIFAR10 = make_classification_dataset(CIFAR10(datadir, train=False, download=True))\n",
    "\n",
    "bm = benchmark_from_datasets(\n",
    "    train=[train_MNIST, train_CIFAR10],\n",
    "    test=[test_MNIST, test_CIFAR10]\n",
    ")\n",
    "\n",
    "print(f\"{bm.train_stream.name} - len {len(bm.train_stream)}\")\n",
    "print(f\"{bm.test_stream.name} - len {len(bm.test_stream)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31ab74",
   "metadata": {},
   "source": [
    "we can also split a validation stream from the training stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d69ff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original training samples = 45000\n",
      "new training samples = 33750\n",
      "validation samples = 15000\n"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks.scenarios.dataset_scenario import benchmark_with_validation_stream\n",
    "\n",
    "\n",
    "print(f\"original training samples = {len(bm.train_stream[0].dataset)}\")\n",
    "\n",
    "bm = benchmark_with_validation_stream(bm, validation_size=0.25)\n",
    "print(f\"new training samples = {len(bm.train_stream[0].dataset)}\")\n",
    "print(f\"validation samples = {len(bm.valid_stream[0].dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc41886",
   "metadata": {},
   "source": [
    "### Experience Attributes\n",
    "\n",
    "The Continual Learning nomenclature is overloaded and quite confusing. Avalanche has its own nomenclature to provide consistent naming across the library.\n",
    "For example:\n",
    "- **Task-awareness**: a model is task-aware if it requires task labels. Avalanche benchmarks can have task labels to support this use case;\n",
    "- **Online**: online streams are streams with small experiences (e.g. 10 samples). They look exactly like their \"large batches\" counterpart, except for the fact that `len(experience.dataset)` is small;\n",
    "- **Boundary-awareness**: a model is boundary-aware if it requires boundary labels. Boundary-free models are also called task-free in the literature (there is not accepted nomenclature for \"boundary-aware\" models). We don't use this nomenclature because task and boundaries are different concepts in Avalanche. Avalanche benchmarks can have boundary labels to support this use case. Even for boundary-free models, Avalanche benchmarks can provide boundary labels to support evaluation metrics that require them;\n",
    "- **Classification**: classification is the most common CL setting. Avalanche adds class labels to experience to simplify the code of the user. Similarly, Avalanche datasets keep track of `targets` to support this use case.\n",
    "\n",
    "Avalanche experiences can be decorated with different attributes depending on the specific setting.\n",
    "Classic benchmarks already provide the attributes you need. We will see some examples of attributes and generators in the remaining part of this tutorial.\n",
    "\n",
    "One general aspects of experience attributes is that they may not always be available. Sometimes, a model can use task labels during training but not at evaluation time. Other times, the model should never use task lavels but you may still need them for evaluation purposes (to compute task-aware metrics). Avalanche experience have different modalities:\n",
    "- training mode\n",
    "- evaluation mode\n",
    "- logging mode\n",
    "\n",
    "Each modality can provide access or mask some of the experience attributes. This mechanism allows you to easily add private attributes to the experience for logging purposes while ensuring that the model will not cheat by using that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd4ee01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't access current_experience during training\n",
      "can't access current_experience during evaluation\n",
      "Experience 0 - [8, 2]\n"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks.scenarios.generic_scenario import MaskedAttributeError\n",
    "\n",
    "\n",
    "bm = SplitMNIST(n_experiences=5)\n",
    "\n",
    "exp = bm.train_stream[0]\n",
    "# current experience is the position of the experience in the stream.\n",
    "# It must never be used during training or evaluation\n",
    "# if you try to use it will fail with a MaskedAttributeError\n",
    "\n",
    "try:\n",
    "    # exp.train() returns the experience in training mode\n",
    "    print(f\"Experience {exp.train().current_experience}\")\n",
    "except MaskedAttributeError as e:\n",
    "    print(\"can't access current_experience during training\")\n",
    "\n",
    "try:\n",
    "    # exp.eval() returns the experience in evaluation mode\n",
    "    print(f\"Experience {exp.eval().current_experience}\")\n",
    "except MaskedAttributeError as e:\n",
    "    print(\"can't access current_experience during evaluation\")\n",
    "\n",
    "# exp.logging() returns the experience in logging mode\n",
    "# everything is available during logging\n",
    "print(f\"Experience {exp.logging().current_experience}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a878e",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "\n",
    "classification benchmarks follow the `ClassesTimeline` protocol and provide attributes about the classes in the stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc54d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience 0\n",
      "Classes in this experience: {9, 2}\n",
      "Previous classes: {9, 2}\n",
      "Future classes: {0, 1, 3, 4, 5, 6, 7, 8}\n"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks.scenarios.supervised import class_incremental_benchmark\n",
    "\n",
    "datadir = default_dataset_location('mnist')\n",
    "train_MNIST = make_classification_dataset(MNIST(datadir, train=True, download=True))\n",
    "test_MNIST = make_classification_dataset(MNIST(datadir, train=False, download=True))\n",
    "\n",
    "# a class-incremental split\n",
    "# 5 experiences, 2 classes per experience\n",
    "bm = class_incremental_benchmark({'train': train_MNIST, 'test': test_MNIST}, num_experiences=5)\n",
    "\n",
    "exp = bm.train_stream[0]\n",
    "print(f\"Experience {exp.current_experience}\")\n",
    "print(f\"Classes in this experience: {exp.classes_in_this_experience}\")\n",
    "print(f\"Previous classes: {exp.classes_seen_so_far}\")\n",
    "print(f\"Future classes: {exp.future_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b64429c",
   "metadata": {},
   "source": [
    "#### Task Labels\n",
    "\n",
    "task-aware benchmarks add task labels, following the `TaskAware` protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca6b45d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "AvalancheDataset already has task labels. Use `benchmark_from_datasets` instead or set `reset_task_labels=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\w-32\\Workspace\\avalanche\\notebooks\\from-zero-to-hero-tutorial\\03_benchmarks.ipynb Cell 19\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/w-32/Workspace/avalanche/notebooks/from-zero-to-hero-tutorial/03_benchmarks.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m bm \u001b[39m=\u001b[39m class_incremental_benchmark({\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: train_MNIST, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m: test_MNIST}, num_experiences\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/w-32/Workspace/avalanche/notebooks/from-zero-to-hero-tutorial/03_benchmarks.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# we take the class-incremental benchmark defined above and\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/w-32/Workspace/avalanche/notebooks/from-zero-to-hero-tutorial/03_benchmarks.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# add an incremental task label to each experience\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/w-32/Workspace/avalanche/notebooks/from-zero-to-hero-tutorial/03_benchmarks.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# each sample will have its own task label\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/w-32/Workspace/avalanche/notebooks/from-zero-to-hero-tutorial/03_benchmarks.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m bm \u001b[39m=\u001b[39m task_incremental_benchmark(bm)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/w-32/Workspace/avalanche/notebooks/from-zero-to-hero-tutorial/03_benchmarks.ipynb#X41sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m exp \u001b[39min\u001b[39;00m bm\u001b[39m.\u001b[39mtrain_stream:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/w-32/Workspace/avalanche/notebooks/from-zero-to-hero-tutorial/03_benchmarks.ipynb#X41sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExperience \u001b[39m\u001b[39m{\u001b[39;00mexp\u001b[39m.\u001b[39mcurrent_experience\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\w-32\\Workspace\\avalanche\\notebooks\\from-zero-to-hero-tutorial\\../..\\avalanche\\benchmarks\\scenarios\\task_aware.py:139\u001b[0m, in \u001b[0;36mtask_incremental_benchmark\u001b[1;34m(bm, reset_task_labels)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mfor\u001b[39;00m eid, exp \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(stream):\n\u001b[0;32m    138\u001b[0m     \u001b[39mif\u001b[39;00m has_task_labels(exp\u001b[39m.\u001b[39mdataset) \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m reset_task_labels):\n\u001b[1;32m--> 139\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAvalancheDataset already has task labels. Use `benchmark_from_datasets` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39minstead or set `reset_task_labels=True`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    141\u001b[0m     tls \u001b[39m=\u001b[39m TaskLabels(ConstantSequence(eid, \u001b[39mlen\u001b[39m(exp\u001b[39m.\u001b[39mdataset)))\n\u001b[0;32m    142\u001b[0m     new_dd \u001b[39m=\u001b[39m exp\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mupdate_data_attribute(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtargets_task_labels\u001b[39m\u001b[39m\"\u001b[39m, new_value\u001b[39m=\u001b[39mtls)\n",
      "\u001b[1;31mValueError\u001b[0m: AvalancheDataset already has task labels. Use `benchmark_from_datasets` instead or set `reset_task_labels=True`."
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks.scenarios.supervised import class_incremental_benchmark\n",
    "from avalanche.benchmarks.scenarios.task_aware import task_incremental_benchmark\n",
    "\n",
    "bm = class_incremental_benchmark({'train': train_MNIST, 'test': test_MNIST}, num_experiences=5)\n",
    "\n",
    "# we take the class-incremental benchmark defined above and\n",
    "# add an incremental task label to each experience\n",
    "# each sample will have its own task label\n",
    "bm = task_incremental_benchmark(bm)\n",
    "\n",
    "for exp in bm.train_stream:\n",
    "    print(f\"Experience {exp.current_experience}\")\n",
    "\n",
    "    # in Avalanche an experience may have multiple task labels\n",
    "    # if the samples in its dataset come from different tasks\n",
    "    # here we just have one task label per experience\n",
    "    print(f\"\\tTask labels: {exp.task_labels}\")\n",
    "\n",
    "    # samples are now triplets <x, y, task_id>\n",
    "    print(f\"\\tSample: {exp.dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d85699",
   "metadata": {},
   "source": [
    "#### Online\n",
    "\n",
    "To define online streams we need two things:\n",
    "- a mechanism to split a larger stream\n",
    "- attribute that indicate the boundaries (if necessary)\n",
    "\n",
    "This is how you do it in Avalanche:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bbff90e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "split_online_stream() missing 1 required positional argument: 'online_benchmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\w-32\\Workspace\\avalanche\\notebooks\\from-zero-to-hero-tutorial\\03_benchmarks.ipynb Cell 21\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/w-32/Workspace/avalanche/notebooks/from-zero-to-hero-tutorial/03_benchmarks.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m bm \u001b[39m=\u001b[39m class_incremental_benchmark({\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: train_MNIST, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m: test_MNIST}, num_experiences\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/w-32/Workspace/avalanche/notebooks/from-zero-to-hero-tutorial/03_benchmarks.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# we split the training stream into online experiences\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/w-32/Workspace/avalanche/notebooks/from-zero-to-hero-tutorial/03_benchmarks.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# we don't need to split validation/test streams\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/w-32/Workspace/avalanche/notebooks/from-zero-to-hero-tutorial/03_benchmarks.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# it's actually more convenient to keep them whole to compute the metrics\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/w-32/Workspace/avalanche/notebooks/from-zero-to-hero-tutorial/03_benchmarks.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m online_train_stream \u001b[39m=\u001b[39m split_online_stream(bm\u001b[39m.\u001b[39;49mtrain_stream, experience_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/w-32/Workspace/avalanche/notebooks/from-zero-to-hero-tutorial/03_benchmarks.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m exp \u001b[39min\u001b[39;00m online_train_stream:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/w-32/Workspace/avalanche/notebooks/from-zero-to-hero-tutorial/03_benchmarks.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExperience \u001b[39m\u001b[39m{\u001b[39;00mexp\u001b[39m.\u001b[39mcurrent_experience\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: split_online_stream() missing 1 required positional argument: 'online_benchmark'"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks.scenarios.online import split_online_stream\n",
    "\n",
    "\n",
    "bm = class_incremental_benchmark({'train': train_MNIST, 'test': test_MNIST}, num_experiences=5)\n",
    "\n",
    "# we split the training stream into online experiences\n",
    "# we don't need to split validation/test streams\n",
    "# it's actually more convenient to keep them whole to compute the metrics\n",
    "online_train_stream = split_online_stream(bm.train_stream, experience_size=10)\n",
    "\n",
    "for exp in online_train_stream:\n",
    "    print(f\"Experience {exp.current_experience}\")\n",
    "    print(f\"\\tsize: {len(exp.dataset)}\")\n",
    "\n",
    "    # in a training loop, here you would train on the online_train_stream\n",
    "    # here you would test on bm.valid_stream or bm.test_stream "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42a39ec6",
   "metadata": {},
   "source": [
    "This completes the \"_Benchmark_\" tutorial for the \"_From Zero to Hero_\" series. We hope you enjoyed it!\n",
    "\n",
    "## ü§ù Run it on Google Colab\n",
    "\n",
    "You can run _this chapter_ and play with it on Google Colaboratory: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContinualAI/avalanche/blob/master/notebooks/from-zero-to-hero-tutorial/03_benchmarks.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
