{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "description: Continual Learning Algorithms Prototyping Made Easy\n",
    "---\n",
    "# Training\n",
    "\n",
    "Welcome to the \"_Training_\" tutorial of the \"_From Zero to Hero_\" series. In this part we will present the functionalities offered by the `training` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ContinualAI/avalanche.git\r\n",
      "  Cloning https://github.com/ContinualAI/avalanche.git to /tmp/pip-req-build-f00959wq\r\n",
      "  Running command git clone -q https://github.com/ContinualAI/avalanche.git /tmp/pip-req-build-f00959wq\r\n",
      "^C\r\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\r\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3 is available.\r\n",
      "You should consider upgrading via the '/home/carta/anaconda3/envs/avalanche/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/ContinualAI/avalanche.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💪 The Training Module\n",
    "\n",
    "The `training` module in _Avalanche_ is designed with modularity in mind. Its main goals are to:\n",
    "\n",
    "1. provide a set of popular **continual learning baselines** that can be easily used to run experimental comparisons;\n",
    "2. provide simple abstractions to **create and run your own strategy** as efficiently and easily as possible starting from a couple of basic building blocks we already prepared for you.\n",
    "\n",
    "At the moment, the `training` module includes two main components:\n",
    "\n",
    "* **Strategies**: these are popular baselines already implemented for you which you can use for comparisons or as base classes to define a custom strategy.\n",
    "* **Plugins**: these are classes that allow to add some specific behaviour to your own strategy. The plugin system allows to define reusable components which can be easily combined together (e.g. a replay strategy, a regularization strategy). They are also used to automatically manage logging and evaluation.\n",
    "\n",
    "Keep in mind that many Avalanche components are independent from Avalanche strategies. If you already have your own strategy which does not use Avalanche, you can use Avalanche's benchmarks, models, data loaders, and metrics without ever looking at Avalanche's strategies.\n",
    "\n",
    "## 📈 How to Use Strategies & Plugins\n",
    "\n",
    "If you want to compare your strategy with other classic continual learning algorithm or baselines, in _Avalanche_ you can instantiate a strategy with a couple lines of code.\n",
    "\n",
    "### Strategy Instantiation\n",
    "Most strategies require only 3 mandatory arguments:\n",
    "- **model**: this must be a `torch.nn.Module`.\n",
    "- **optimizer**: `torch.optim.Optimizer` already initialized on your `model`.\n",
    "- **loss**: a loss function such as those in `torch.nn.functional`.\n",
    "\n",
    "Additional arguments are optional and allow you to customize training (batch size, epochs, ...) or strategy-specific parameters (buffer size, regularization strength, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training.strategies import Naive, CWRStar, Replay, GDumb, Cumulative, LwF, GEM, AGEM, EWC\n",
    "\n",
    "model = SimpleMLP(num_classes=10)\n",
    "optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = CrossEntropyLoss()\n",
    "cl_strategy = Naive(\n",
    "    model, optimizer, criterion, \n",
    "    train_mb_size=100, train_epochs=4, eval_mb_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Evaluation\n",
    "\n",
    "Each strategy object offers two main methods: `train` and `eval`. Both of them, accept either a _single experience_(`Experience`) or a _list of them_, for maximum flexibility.\n",
    "\n",
    "We can train the model continually by iterating over the `train_stream` provided by the scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carta/anaconda3/envs/avalanche/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [5, 6]\n",
      "-- >> Start of training phase << --\n",
      "-- Starting training on experience 0 (Task 0) from train stream --\n",
      "100%|██████████| 114/114 [00:07<00:00, 15.89it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.3855\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8944\n",
      "100%|██████████| 114/114 [00:02<00:00, 52.19it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.1017\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9699\n",
      "100%|██████████| 114/114 [00:02<00:00, 52.87it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0860\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9728\n",
      "100%|██████████| 114/114 [00:02<00:00, 50.59it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0775\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9747\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 19/19 [00:00<00:00, 64.88it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0662\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.9805\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 22/22 [00:00<00:00, 66.14it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 7.6797\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 20/20 [00:00<00:00, 66.31it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 10.3700\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 69.25it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 10.1055\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 68.72it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 8.3632\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 7.4240\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.1814\n",
      "Start of experience:  1\n",
      "Current Classes:  [1, 2]\n",
      "-- >> Start of training phase << --\n",
      "-- Starting training on experience 1 (Task 0) from train stream --\n",
      "100%|██████████| 127/127 [00:02<00:00, 52.52it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.5995\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8753\n",
      "100%|██████████| 127/127 [00:02<00:00, 52.37it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0637\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9818\n",
      "100%|██████████| 127/127 [00:02<00:00, 52.68it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0509\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9855\n",
      "100%|██████████| 127/127 [00:02<00:00, 48.24it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0437\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9883\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 19/19 [00:00<00:00, 64.95it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 7.3342\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 22/22 [00:00<00:00, 64.25it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0298\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.9935\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 20/20 [00:00<00:00, 62.23it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 10.3561\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 67.33it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 8.7540\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 61.34it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 8.1414\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 6.7907\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.2153\n",
      "Start of experience:  2\n",
      "Current Classes:  [0, 8]\n",
      "-- >> Start of training phase << --\n",
      "-- Starting training on experience 2 (Task 0) from train stream --\n",
      "100%|██████████| 118/118 [00:02<00:00, 50.64it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8106\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8414\n",
      "100%|██████████| 118/118 [00:02<00:00, 51.14it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0629\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9822\n",
      "100%|██████████| 118/118 [00:02<00:00, 50.92it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0493\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9862\n",
      "100%|██████████| 118/118 [00:02<00:00, 50.01it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0411\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9885\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 19/19 [00:00<00:00, 65.70it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 6.4123\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 22/22 [00:00<00:00, 65.03it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 5.8889\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 20/20 [00:00<00:00, 65.70it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0285\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.9933\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 63.60it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 8.8877\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 66.08it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 7.6361\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.7972\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.1941\n",
      "Start of experience:  3\n",
      "Current Classes:  [9, 3]\n",
      "-- >> Start of training phase << --\n",
      "-- Starting training on experience 3 (Task 0) from train stream --\n",
      "100%|██████████| 121/121 [00:02<00:00, 48.92it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9070\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8080\n",
      "100%|██████████| 121/121 [00:02<00:00, 51.42it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.1182\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9632\n",
      "100%|██████████| 121/121 [00:02<00:00, 52.20it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0958\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9707\n",
      "100%|██████████| 121/121 [00:02<00:00, 52.18it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0832\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9751\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 19/19 [00:00<00:00, 67.83it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 7.1905\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 22/22 [00:00<00:00, 68.87it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 6.0013\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 20/20 [00:00<00:00, 67.03it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 6.6719\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0041\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 67.86it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0617\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.9787\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 69.33it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 9.6518\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.8869\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.1984\n",
      "Start of experience:  4\n",
      "Current Classes:  [4, 7]\n",
      "-- >> Start of training phase << --\n",
      "-- Starting training on experience 4 (Task 0) from train stream --\n",
      "100%|██████████| 122/122 [00:02<00:00, 52.34it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9756\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8031\n",
      "100%|██████████| 122/122 [00:02<00:00, 53.56it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0948\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9741\n",
      "100%|██████████| 122/122 [00:02<00:00, 54.31it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0672\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9813\n",
      "100%|██████████| 122/122 [00:02<00:00, 54.24it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0571\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9844\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 19/19 [00:00<00:00, 61.36it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 7.6036\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 22/22 [00:00<00:00, 67.69it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 5.6888\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 20/20 [00:00<00:00, 66.66it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 6.6404\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0026\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 69.65it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 6.9593\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0857\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 69.99it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0403\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.9866\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.3502\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.2161\n"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "\n",
    "# scenario\n",
    "benchmark = SplitMNIST(n_experiences=5, seed=1)\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in benchmark.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    cl_strategy.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    results.append(cl_strategy.eval(benchmark.test_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Plugins\n",
    "\n",
    "Most continual learning strategies follow roughly the same training/evaluation loops, i.e. a simple naive strategy (a.k.a. finetuning) augmented with additional behavior to counteract catastrophic forgetting. The plugin systems in Avalanche is designed to easily augment continual learning strategies with custom behavior, without having to rewrite the training loop from scratch. Avalanche strategies accept an optional list of `plugins` that will be executed during the training/evaluation loops.\n",
    "\n",
    "For example, early stopping is implemented as a plugin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from avalanche.training.plugins import EarlyStoppingPlugin\n",
    "\n",
    "strategy = Naive(\n",
    "    model, optimizer, criterion,\n",
    "    plugins=[EarlyStoppingPlugin(patience=10, val_stream_name='train')])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In Avalanche, most continual learning strategies are implemented using plugins, which makes it easy to combine them together. For example, it is extremely easy to create a hybrid strategy that combines replay and EWC together by passing the appropriate `plugins` list to the `BaseStrategy`:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from avalanche.training.strategies import BaseStrategy\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin\n",
    "\n",
    "replay = ReplayPlugin(mem_size=100)\n",
    "ewc = EWCPlugin(ewc_lambda=0.001)\n",
    "strategy = BaseStrategy(\n",
    "    model, optimizer, criterion,\n",
    "    plugins=[replay, ewc])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Beware that most strategy plugins modify the internal state. As a result, not all the strategy plugins can be combined together. For example, it does not make sense to use multiple replay plugins since they will try to modify the same strategy variables (mini-batches, dataloaders), and therefore they will be in conflict."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 📝 A Look Inside Avalanche Strategies\n",
    "\n",
    "If you arrived at this point you already know how to use Avalanche strategies and are ready to use it. However, before making your own strategies you need to understand a little bit the internal implementation of the training and evaluation loops.\n",
    "\n",
    "In _Avalanche_ you can customize a strategy in 2 ways:\n",
    "\n",
    "1. **Plugins**: Most modifications can be defined by _augmenting_ the basic training and evaluation loops. The easiest way to define a custom strategy such as a regularization or replay strategy, is to define it as a custom plugin. The advantage of this approach is that you can easily reuse your plugin with any other strategy. The disadvantage is that in order to do so you need to understand the `BaseStrategy` loop, which can be a bit complex at first.\n",
    "2. **Subclassing**: In _Avalanche_, continual learning strategies inherit from the `BaseStrategy`, which provides generic training and evaluation loops. Most methods can be safely overridden (with some caveats that we will see later).\n",
    "\n",
    "Keep in mind that if you already have a continual learning strategy that does not use _Avalanche_, you can always most Avalanche components such as `benchmarks`, `evaluation`, and `models` components without using _Avalanche_'s strategies!\n",
    "\n",
    "### Training and Evaluation Loops\n",
    "\n",
    "As we already mentioned, _Avalanche_ strategies inherit from `BaseStrategy`. This strategy provides:\n",
    "\n",
    "1. Basic _Training_ and _Evaluation_ loops which define a naive (finetuning) strategy.\n",
    "2. _Callback_ points, which are used to call the plugins at a specific moments during the loop's execution.\n",
    "3. A set of variables representing the state of the loops (current model, data, mini-batch, predictions, ...) which allows plugins and child classes to easily manipulate the state of the training loop.\n",
    "\n",
    "The training loop has the following structure:\n",
    "```text\n",
    "train\n",
    "    before_training\n",
    "\n",
    "    before_train_dataset_adaptation\n",
    "    train_dataset_adaptation\n",
    "    after_train_dataset_adaptation\n",
    "    make_train_dataloader\n",
    "    model_adaptation\n",
    "    make_optimizer\n",
    "    before_training_exp  # for each exp\n",
    "        before_training_epoch  # for each epoch\n",
    "            before_training_iteration  # for each iteration\n",
    "                before_forward\n",
    "                after_forward\n",
    "                before_backward\n",
    "                after_backward\n",
    "            after_training_iteration\n",
    "            before_update\n",
    "            after_update\n",
    "        after_training_epoch\n",
    "    after_training_exp\n",
    "    after_training\n",
    "```\n",
    "\n",
    "The evaluation loop is similar:\n",
    "```text\n",
    "eval\n",
    "    before_eval\n",
    "    before_eval_dataset_adaptation\n",
    "    eval_dataset_adaptation\n",
    "    after_eval_dataset_adaptation\n",
    "    make_eval_dataloader\n",
    "    model_adaptation\n",
    "    before_eval_exp  # for each exp\n",
    "        eval_epoch  # we have a single epoch in evaluation mode\n",
    "            before_eval_iteration  # for each iteration\n",
    "                before_eval_forward\n",
    "                after_eval_forward\n",
    "            after_eval_iteration\n",
    "    after_eval_exp\n",
    "    after_eval\n",
    "```\n",
    "\n",
    "Methods starting with `before/after` are the methods responsible for calling the plugins.\n",
    "Notice that before the start of each experience during training we have several phases:\n",
    "- *dataset adaptation*: This is the phase where the training data can be modified by the strategy, for example by adding other samples from a separate buffer.\n",
    "- *dataloader initialization*: Initialize the data loader. Many strategies (e.g. replay) use custom dataloaders to balance the data.\n",
    "- *model adaptation*: Here, the dynamic models (see the `models` tutorial) are updated by calling their `adaptation` method.\n",
    "- *optimizer initialization*: After the model has been updated, the optimizer should also be updated to ensure that the new parameters are optimized.\n",
    "\n",
    "## How to Write a Plugin\n",
    "Plugins provide a simple solution to define a new strategy by augmenting the behavior of another strategy (typically a naive strategy). This approach reduces the overhead and code duplication, **improving code readability and prototyping speed**.\n",
    "\n",
    "Creating a plugin is straightforward. You create a class which inherits from `StrategyPlugin` and implements the callbacks that you need. The exact callback to use depend on your strategy. You can use the loop shown above to understand what callbacks you need to use. For example, we show below a simple replay plugin that uses `after_training_exp` to update the buffer after each training experience, and the `before_training_exp` to customize the dataloader. Notice that `before_training_exp` is executed after `make_train_dataloader`, which means that the `BaseStrategy` already updated the dataloader. If we used another callback, such as `before_train_dataset_adaptation`, our dataloader would have been overwritten by the `BaseStrategy`. Plugin methods always receive the `strategy` as an argument, so they can access and modify the strategy's state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- >> Start of training phase << --\n",
      "-- Starting training on experience 0 (Task 0) from train stream --\n",
      "100%|██████████| 89/89 [00:02<00:00, 43.11it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.1852\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9414\n",
      "-- Starting training on experience 1 (Task 0) from train stream --\n",
      "100%|██████████| 199/199 [00:04<00:00, 43.03it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2966\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9331\n",
      "-- Starting training on experience 2 (Task 0) from train stream --\n",
      "100%|██████████| 184/184 [00:04<00:00, 42.73it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.3202\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9112\n",
      "-- Starting training on experience 3 (Task 0) from train stream --\n",
      "100%|██████████| 189/189 [00:04<00:00, 42.45it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.4172\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8869\n",
      "-- Starting training on experience 4 (Task 0) from train stream --\n",
      "100%|██████████| 190/190 [00:04<00:00, 42.81it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.4347\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8835\n",
      "-- >> End of training phase << --\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'Top1_Acc_Epoch/train_phase/train_stream/Task000': 0.8835060556873517,\n 'Loss_Epoch/train_phase/train_stream/Task000': 0.4346607615425082,\n 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000': 0.0,\n 'Loss_Exp/eval_phase/test_stream/Task000/Exp000': 7.603605347710687,\n 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp001': 0.0,\n 'Loss_Exp/eval_phase/test_stream/Task000/Exp001': 5.688833829275078,\n 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp002': 0.00255885363357216,\n 'Loss_Exp/eval_phase/test_stream/Task000/Exp002': 6.640405380127984,\n 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp003': 0.08568598315998019,\n 'Loss_Exp/eval_phase/test_stream/Task000/Exp003': 6.959345254524915,\n 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp004': 0.9865671641791045,\n 'Loss_Exp/eval_phase/test_stream/Task000/Exp004': 0.0402771831334879,\n 'Top1_Acc_Stream/eval_phase/test_stream/Task000': 0.2161,\n 'Loss_Stream/eval_phase/test_stream/Task000': 5.350160012105806}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from avalanche.benchmarks.utils.data_loader import ReplayDataLoader\n",
    "from avalanche.training.plugins import StrategyPlugin\n",
    "from avalanche.training.storage_policy import ReservoirSamplingBuffer\n",
    "\n",
    "\n",
    "class ReplayP(StrategyPlugin):\n",
    "\n",
    "    def __init__(self, mem_size):\n",
    "        \"\"\" A simple replay plugin with reservoir sampling. \"\"\"\n",
    "        super().__init__()\n",
    "        self.buffer = ReservoirSamplingBuffer(max_size=mem_size)\n",
    "\n",
    "    def before_training_exp(self, strategy: \"BaseStrategy\",\n",
    "                            num_workers: int = 0, shuffle: bool = True,\n",
    "                            **kwargs):\n",
    "        \"\"\" Use a custom dataloader to combine samples from the current data and memory buffer. \"\"\"\n",
    "        if len(self.buffer.buffer) == 0:\n",
    "            # first experience. We don't use the buffer, no need to change\n",
    "            # the dataloader.\n",
    "            return\n",
    "        strategy.dataloader = ReplayDataLoader(\n",
    "            strategy.adapted_dataset,\n",
    "            self.buffer.buffer,\n",
    "            oversample_small_tasks=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=strategy.train_mb_size,\n",
    "            shuffle=shuffle)\n",
    "\n",
    "    def after_training_exp(self, strategy: \"BaseStrategy\", **kwargs):\n",
    "        \"\"\" Update the buffer. \"\"\"\n",
    "        self.buffer.update(strategy, **kwargs)\n",
    "\n",
    "\n",
    "model = SimpleMLP(num_classes=10)\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = CrossEntropyLoss()\n",
    "#strategy = Naive(model=model, optimizer=optimizer, criterion=criterion, train_mb_size=128,\n",
    "#plugins=[ReplayP(mem_size=2000)])\n",
    "strategy = Replay(model=model, optimizer=optimizer, criterion=criterion, train_mb_size=128, mem_size=2000)\n",
    "strategy.train(benchmark.train_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check `StrategyPlugin`'s documentation for a complete list of the available callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Write a Custom Strategy\n",
    "\n",
    "You can always define a custom strategy by overriding `BaseStrategy` methods.\n",
    "However, There is an important caveat to keep in mind. If you override a method, you must remember to call all the callback's handlers at the appropriate points. For example, `train` calls `before_training` and `after_training` before and after the training loops, respectively. If your strategy strategy does not call them, plugins may not work as expected. The easiest way to avoid mistakes is to start from the `BaseStrategy` method that you want to override and modify it to your own needs without removing the callbacks handling.\n",
    "\n",
    "There is only a single plugin that is always used by default, the `EvaluationPlugin` (see `evaluation` tutorial). This means that if you break callbacks you must log metrics by yourself. This is totally possible but requires some manual work to update, log, and reset each metric, which is done automatically for you by the `BaseStrategy`.\n",
    "\n",
    "`BaseStrategy` provides the global state of the loop in the strategy's attributes, which you can safely use when you override a method. As an example, the `Cumulative` strategy trains a model continually on the union of all the experiences encountered so far. To achieve this, the cumulative strategy overrides `adapt_train_dataset` and updates `self.adapted_dataset' by concatenating all the previous experiences with the current one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from avalanche.benchmarks.utils import AvalancheConcatDataset\n",
    "from avalanche.training import BaseStrategy\n",
    "\n",
    "\n",
    "class Cumulative(BaseStrategy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dataset = None  # cumulative dataset\n",
    "\n",
    "    def train_dataset_adaptation(self, **kwargs):\n",
    "        super().train_dataset_adaptation(**kwargs)\n",
    "        curr_data = self.experience.dataset\n",
    "        if self.dataset is None:\n",
    "            self.dataset = curr_data\n",
    "        else:\n",
    "            self.dataset = AvalancheConcatDataset([self.dataset, curr_data])\n",
    "        self.adapted_dataset = self.dataset.train()\n",
    "\n",
    "strategy = Cumulative(model=model, optimizer=optimizer, criterion=criterion, train_mb_size=128)\n",
    "strategy.train(benchmark.train_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy, isn't it? :-\\)\n",
    "\n",
    "In general, we recommend to _implement a Strategy via plugins_, if possible. This approach is the easiest to use and requires a minimal knowledge of the `BaseStrategy`. It also allows other people to use your plugin and facilitates interoperability among different strategies.\n",
    "\n",
    "For example, replay strategies can be implemented as a custom strategy of the `BaseStrategy` or as plugins. However, creating a plugin is better because it allows to use our replay strategy in conjunction with other strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the \"_Training_\" chapter for the \"_From Zero to Hero_\" series. We hope you enjoyed it!\n",
    "\n",
    "## 🤝 Run it on Google Colab\n",
    "\n",
    "You can run _this chapter_ and play with it on Google Colaboratory: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContinualAI/colab/blob/master/notebooks/avalanche/3.-training.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}