from typing import List, Callable, Sequence, Iterable

import torch

from benchmarks import Experience
from benchmarks.scenarios.generic_scenario import CLExperience, CLStream, \
    LazyCLStream, ExperienceAttribute
from benchmarks.utils import AvalancheSubset, AvalancheDataset


def fixed_size_experience_split(
    experience: CLExperience,
    experience_size: int,
    shuffle: bool = True,
    drop_last: bool = False,
):
    """Returns a lazy stream generated by splitting an experience into smaller
    ones.

    Splits the experience in smaller experiences of size `experience_size`.

    :param experience: The experience to split.
    :param experience_size: The experience size (number of instances).
    :param shuffle: If True, instances will be shuffled before splitting.
    :param drop_last: If True, the last mini-experience will be dropped if
        not of size `experience_size`
    :return: The list of datasets that will be used to create the
        mini-experiences.
    """

    def gen():
        exp_dataset = experience.dataset
        exp_indices = list(range(len(exp_dataset)))

        if shuffle:
            exp_indices = torch.as_tensor(exp_indices)[
                torch.randperm(len(exp_indices))
            ].tolist()

        init_idx = 0
        while init_idx < len(exp_indices):
            final_idx = init_idx + experience_size  # Exclusive
            if final_idx > len(exp_indices):
                if drop_last:
                    break

                final_idx = len(exp_indices)

            exp = CLExperience()
            exp.origin_experience = ExperienceAttribute(experience)
            exp.dataset = AvalancheSubset(
                    exp_dataset, indices=exp_indices[init_idx:final_idx]
                )
            yield exp
            init_idx = final_idx
    return gen()


def split_online_stream(
    original_stream: CLStream,
    experience_size: int,
    shuffle: bool = False,
    drop_last: bool = False,
    experience_split_strategy: Callable[[CLExperience],
                                        Iterable[CLExperience]] = None
):
    """Split a stream of large batches to create an online stream of small
    mini-batches.

    The resulting stream can be used for Online Continual Learning (OCL)
    scenarios (or data-incremental, or other online-based settings).

    For efficiency reasons, the resulting stream is an iterator, generating
    experience on-demand.

    :param original_stream: The stream with the original data.
    :param experience_size: The size of the experience, as an int. Ignored
        if `custom_split_strategy` is used.
    :param shuffle: If True, experiences will be split by first shuffling
        instances in each experience. This will use the default PyTorch
        random number generator at its current state. Defaults to False.
        Ignored if `experience_split_strategy` is used.
    :param drop_last: If True, if the last experience doesn't contain
        `experience_size` instances, then the last experience will be dropped.
        Defaults to False. Ignored if `experience_split_strategy` is used.
    :param experience_split_strategy: A function that implements a custom splitting
        strategy. The function must accept an experience and return an
        experience's iterator. Defaults to None, which means
        that the standard splitting strategy will be used (which creates
        experiences of size `experience_size`).
        A good starting to understand the mechanism is to look at the
        implementation of the standard splitting function
        :func:`fixed_size_experience_split_strategy`.
    :return: A lazy online stream with experiences of size `experience_size`.
    """
    if experience_split_strategy is None:
        experience_split_strategy = lambda exp, size: \
            fixed_size_experience_split(exp, size, shuffle, drop_last)

    def exps_iter():
        for exp in original_stream:
            for sub_exp in experience_split_strategy(exp, experience_size):
                yield exp

    return LazyCLStream(
        name=original_stream.name,
        exps_iter=exps_iter(),
        set_stream_info=True
    )
