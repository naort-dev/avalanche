---
description: Protocols and Metrics Code Examples
---

# Evaluation

_Avalanche_ offers significant support for _defining your own eveluation protocol_ (classic or custom metrics, when and on what to test). You can find **examples** related to the benchmarks here:&#x20;

* [Eval Plugin](../../../examples/eval\_plugin.py): _this is a simple example on how to use the Evaluation Plugin (the evaluation controller object)_
* [Confusion Matrix](../../../examples/confusion\_matrix.py): _this example shows how to produce confusion matrix during training and evaluation._
* [CoPE Strategy](../../../examples/cope.py): _this is a simple example on how to use the CoPE plugin. It's an example in the online data incremental setting, where both learning and evaluation is completely task-agnostic._
* [Dataset Inspection](../../../examples/dataset\_inspection.py)_: this is a simple example on how to use the Dataset inspection plugins._
