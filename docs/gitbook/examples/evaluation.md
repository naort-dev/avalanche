---
description: Protocols and Metrics Code Examples
---

# Evaluation

_Avalanche_ offers significant support for _defining your own eveluation protocol_ (classic or custom metrics, when and on what to test) or using **loggers** such as Tensorboard or W\&B.

You can find **examples** related to the benchmarks here:&#x20;

* [Confusion Matrix](../../../examples/confusion\_matrix.py): this example shows how to produce confusion matrix during training and evaluation.
* [CoPE Strategy](../../../examples/cope.py):clap::clap::clap::clap: this is a simple example on how to use the CoPE plugin. It's an example in the online data incremental setting, where both learning and evaluation is completely task-agnostic.
