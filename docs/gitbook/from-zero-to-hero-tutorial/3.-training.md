---
description: Continual Learning Algorithms Prototyping Made Easy
---

# 3. Training

Welcome to the "_Training_" tutorial of the "_From Zero to Hero_" series. In this part we will present the functionalities offered by the `training` module.

## üí™ The Training Module

The `training` module in _Avalanche_ is build heavily on modularity and its main goals are two:

1. provide a set of standard **continual learning baselines** that can be easily run for comparison; 
2. provide the necessary utilities to **create and run your own strategy** as efficiently and easy as possible.

At the moment the `training` module offers four submodule:

* **Strategies**: it containes the collection of pre-implemented baselines you can use for comparisons.
* **Skeletons**: basic classes and interfaces for modular strategy building.
* **Templates**: ready-to-use classes template for implement your strategy with ease.
* **Plugins**: plugins are modules implementing some specific \(and oftern reusable across strategies\) behaviours you can attach to your own strategy.

## ü§ñ Strategies

If you want to compare your strategy with other classic continual learning algorithms or baselines, in _Avalanche_ this is as simply as instantiate an object. Please note that at the moment only the _Naive_ baseline is supported.

```python
from avalanche.extras.models import SimpleMLP
from avalanche.training.strategies import Naive

model = SimpleMLP(num_classes=10)
cl_strategy = Naive(
    model, 'classifier', SGD(model.parameters(), lr=0.001, momentum=0.9),
    CrossEntropyLoss(), train_mb_size=100, train_epochs=4, test_mb_size=100
)
```

## üìù Create your Strategy

In _Avalanche_ you can build your own strategy in 3 modalities \(of increasing complexity level\):

1. **Beginner Mode**: This is the simplest way to build your own strategy, where you only have to create a python class that implements the main `train` and `test` methods. However, in this case, all the plugins utilities \(like the evaluation ones that we'll see in the follow up tutorial\) cannot be automatically integrated in your algorithm. You should integrate them "_manually_".
2. **Intermediate Mode**: At this level _Avalanche_ offers a sophisticated API to define your strategy based on what we call **flows**. _Training and Testing Flows_ are ordered sequences of functions to be called when the _train_ and _test_ methods are triggered. These function have been already implemented for you in their basic form so you can define a new strategy simply by specializing a few of them. All the evaluation utilities are already integrated in the training and testing flows.
3. **Advanced Mode**: If the standard training and testing flows, for some reason, do not fit in your ideal continual learning strategy, you're free to change them as well!

In general, we suggest to work with the _Intermediate Mode_, which offers in our opinion the right level of automation at a resonable complexity, even for _Avalanche_ "_newbies_". But let's see an example for each of those modalities!

### Benginner Mode

Let's define our Continual Learning algorithm "_MyStrategy_" as a simple python class:

```python
class MyStrategy():
    """My Basic Strategy"""

    def __init__(self, model, optimizer, criterion):
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion

    def train(self, step_info):
        # here you can implement your own training loop for each step (i.e. 
        # batch or task).

        train_dataset, t = step_info.current_training_set()
        train_data_loader = DataLoader(
            train_dataset, num_workers=4, batch_size=128
        )

        for epoch in range(1):
            for mb in train_data_loader:
                # you magin here...
                pass

    def test(self, step_info):
        # here you can implement your own test loop for each step (i.e. 
        # batch or task).

        test_dataset, t = step_info.step_specific_test_set(0)
        test_data_loader = DataLoader(
            test_dataset, num_workers=4, batch_size=128
        )

        # test here
```

Then, we can use our strategy as we would do for the pre-implemented ones. However, as previously hinted, some evaluation utilities will be note included automatically and should be called by hand. Please refer to the follow-up notebook for more details about what the `evaluation` module can offer.

```python
from avalanche.benchmarks.scenarios import DatasetPart, NCBatchInfo
from avalanche.benchmarks.generators import NCScenario
from avalanche.extras.models import SimpleMLP

# --- CONFIG
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
n_batches = 5
# ---------

# --- TRANSFORMATIONS
train_transform = transforms.Compose([
    RandomCrop(28, padding=4),
    ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
    ])
test_transform = transforms.Compose([
    ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
# ---------

# --- SCENARIO CREATION
mnist_train = MNIST('./data/mnist', train=True,
                    download=True, transform=train_transform)
mnist_test = MNIST('./data/mnist', train=False,
                       download=True, transform=test_transform)
nc_scenario = NCScenario(
    mnist_train, mnist_test, n_batches, shuffle=True, seed=1234,
    task_labels=False)
# ---------

# MODEL CREATION
model = SimpleMLP(num_classes=nc_scenario.n_classes)

# CREATE THE STRATEGY INSTANCE (NAIVE)
cl_strategy = MyStrategy(
    model, SGD(model.parameters(), lr=0.001, momentum=0.9),
    CrossEntropyLoss())

# TRAINING LOOP
print('Starting experiment...')
results = []
batch_info: NCBatchInfo
for batch_info in nc_scenario:
    print("Start of step ", batch_info.current_step)

    cl_strategy.train(batch_info)
    print('Training completed')

    print('Computing accuracy on the whole test set')
    results.append(cl_strategy.test(batch_info))
```

### Intermediate Mode

At this level _Avalanche_ offers a sophisticated API to define your strategy based on what we called _Training_ and _Testing Flows_.

**These are ordered sequences of functions to be called when the train and test methods are triggered.**

_Why do we need such a sophisticated mechanism for implementing a continual learning algorithm?_

1. Because these function have been already implemented for you in their basic form so you can define a new strategy just by specializing a few of them.
2. Many continual learning strategy follows roughly the same flows.
3. If strategies are defined only "_by difference_" from a standard `Naive` one, we can use them as plugins \(and hence we can combine different approaches without any effort\).

So the added modularity help us **reducing overhead and code duplication**, **improving code readability and prototying speed**.

**Flows**

As we previously mentioned _Training_ and _Testing flows_ are just sequences of functions.

The **standard flows** defined in _Avalanche_ contain the following functions:

* **Training Flow**:

  ```text
    # - BeforeTraining (group)
    # --- set_initial_epoch
    # --- before_training
    # --- make_train_dataset
    # --- adapt_train_dataset
    # --- make_train_dataloader
    # - ModelTraining (group)
    # --- TrainingLoop (group)
    # ----- before_training_epoch
    # ----- TrainingEpoch (group)
    # ------- training_epoch
    # ----- after_training_epoch
    # --- TrainingModelAdaptation (group)
    # ----- set_task_layer
    # --- next_training_epoch
    # - AfterTraining (group)
    # --- after_training
  ```

* **Testing Flow**:

  ```text
    # - BeforeTesting (group)
    # --- before_testing
    # --- set_initial_test_step_id
    # - MultiStepTestLoop (group)
    # --- before_step_testing
    # --- TestingModelAdaptation (group)
    # ----- set_task_layer
    # --- StepTesting (group)
    # ------ make_test_dataset
    # ------ adapt_test_dataset
    # ------ make_test_dataloader
    # ------ ModelTesting (group)
    # --------- before_testing_epoch
    # --------- TestingEpoch (group)
    # ------------ testing_epoch
    # --------- after_testing_epoch
    # --- after_step_testing
    # --- next_testing_step
    # - AfterTesting (group)
    # --- after_testing
  ```

The are all the functions contained in the flows as defined by the `MTDeepLearningStrategy`, the basic class we suggest you to inherit for implementing your continual learning algorithms \(keep in mind that this class provides other callback that are not included in the flow but you can use\).

For this class, all the methods \(that you can of course override\) in the flows are implemented for you with the exception of the _training and testing epoch methods_. So, creating a plain `Naive` algorithm can be done as simply as defining the subclass below:

```python
from avalanche.training.templates import MTDeepLearningStrategy
from avalanche.training.skeletons import TrainingFlow, TestingFlow

class Naive(MTDeepLearningStrategy):
    """
    The simplest (and least effective) Continual Learning strategy. Naive just
    incrementally fine tunes a single model without employing any method
    to contrast the catastrophic forgetting of previous knowledge.

    Naive is easy to set up and its results are commonly used to show the worst
    performing baseline.
    """
    def __init__(self, model, classifier_field,
                 optimizer, criterion,
                 train_mb_size=1, train_epochs=1,
                 test_mb_size=None, device=None,
                 evaluation_protocol=None):
        """
        Creates an instance of the Naive strategy.

        :param model: The model.
        :param classifier_field: The name of the classifier field. Used when
            managing heads in Multi-Task scenarios.
        :param optimizer: The optimizer to use.
        :param criterion: The loss criterion to use.
        :param train_mb_size: The train minibatch size. Defaults to 1.
        :param train_epochs: The number of training epochs. Defaults to 1.
        :param test_mb_size: The test minibatch size. Defaults to 1.
        :param device: The device to use. Defaults to None (cpu).
        :param evaluation_protocol: The evaluation protocol. Defaults to None.
        """
        super(Naive, self).__init__(
            model, classifier_field, train_mb_size=train_mb_size,
            train_epochs=train_epochs, test_mb_size=test_mb_size,
            evaluation_protocol=evaluation_protocol, device=device)

        self.optimizer = optimizer
        self.criterion = criterion
        self.model = model

    @TrainingFlow
    def training_epoch(self, model: Module, train_data_loader,
                       optimizer: Optimizer, criterion: Module, device=None):
        # Number of iterations to run
        epoch_iterations = len(train_data_loader)
        self.update_namespace(epoch_iterations=epoch_iterations)

        # Move model to device
        self.model = model.to(device)
        model = self.model

        # Run an epoch
        for iteration, (train_mb_x, train_mb_y) in enumerate(train_data_loader):
            # Publish some relevant data to the global namespace
            self.update_namespace(iteration=iteration,
                                  train_mb_x=train_mb_x,
                                  train_mb_y=train_mb_y)
            optimizer.zero_grad()

            # Iteration begins
            self.before_training_iteration()

            # Move mini-batch data to device
            train_mb_x = train_mb_x.to(device)
            train_mb_y = train_mb_y.to(device)

            # Forward
            self.before_forward()
            logits = model(train_mb_x)
            self.update_namespace(logits=logits.detach().cpu())
            self.after_forward()

            # Loss
            loss = criterion(logits, train_mb_y)
            self.update_namespace(loss=loss.detach().cpu())

            # Backward
            self.before_backward()
            loss.backward()
            self.after_backward()

            # Update
            self.before_update()
            optimizer.step()
            self.after_update()

            # Iteration end
            self.after_training_iteration()

    @TestingFlow
    def testing_epoch(self, model: Module, test_data_loader,
                      criterion: Module, device=None):
        epoch_iterations = len(test_data_loader)
        self.model = model = model.to(device)
        self.update_namespace(epoch_iterations=epoch_iterations)

        for iteration, (test_mb_x, test_mb_y) in enumerate(test_data_loader):
            self.update_namespace(iteration=iteration,
                                  test_mb_x=test_mb_x,
                                  test_mb_y=test_mb_y)
            # Iteration begins
            self.before_test_iteration()

            # Move mini-batch data to device
            test_mb_x = test_mb_x.to(device)
            test_mb_y = test_mb_y.to(device)

            # Forward
            self.before_test_forward()
            test_logits = model(test_mb_x)
            self.update_namespace(test_logits=test_logits.detach().cpu())
            self.after_test_forward()

            # Loss
            test_loss = criterion(test_logits, test_mb_y)
            self.update_namespace(test_loss=test_loss.detach().cpu())

            # Iteration end
            self.after_test_iteration()
```

**Namespaces**

As you could have noticed from the previous examples, parameters are automatically passed as inputs using the decorators _TestingFlow_ _TestingFlow_. The flow will inject the correct parameter values by looking at different locations:

* **Arguments Namespace**: a parameter with the same name is searched in the arguments passed to the flow. Considering that a strategy part can call other parts \(to provide a callback or to obtain results\), parameters passed to previous method calls in the stack are searched for, too;
* **Results Namespace**: a global flow namespace exists where each part may publish its results. These values are stored and used for parameter injection and, like the arguments passed to the flow, they are discarded after each flow execution. The good part of it is that this namespace is shared with submodules, which makes it easier to modularize some common behaviours. When a part publishes a result, values with the same name found in the first group are discarded;
* **Self Namespace**: fields of the strategy class or any of its attached submodules are searched for. It goes without saying, those are the only namespace values that are persisted across different flow executions. Fields starting with "\_" are not considered. Also, fields whose values are instances of `StrategyFlow` or `FlowGroup` are not considered, as they would pollute the namespace.

All those values form a **global namespace**. For values with the same, elements or the first group take precedence over the ones in the second group. Values from the second group take precedence over the ones in the last group.

At any moment you can find out all the paramers you can use as follows:

```python
# let's say this is the method you want to specialize
def my_callback_to_specialize(**kwargs):

    # you can just print the kwargs to collect all the parameters 
    # available in the namespace.
    print(kwargs)

my_callback_to_specialize(x=1, y=2, z=3)
```

For convinience, we also report in the table below the signature of each methods and the additional parameters you can get from the namespace, at least in the basic version of the `MTDeepLearningStrategy` class you can inherit and fully customize.

For the **Training Flow**:

| Methods | Arguments Namespace |  |  |
| :--- | :---: | :--- | :--- |
| _set\_initial\_epoch_ | ‚úñ |  |  |
| _before\_training_ | ‚úñ |  |  |
| _make\_train\_dataset_ | step\_info |  |  |
| _adapt\_train\_dataset_ | ‚úñ |  |  |
| _make\_train\_dataloader_ | train\_dataset, num\_workers, train\_mb\_size |  |  |
| _before\_training\_epoch_ | ‚úñ |  |  |
| _training\_epoch_ | ‚úñ |  |  |
| _after\_training\_epoch_ | ‚úñ |  |  |
| _set\_task\_layer_ | model, classifier\_field, step\_info, step\_id |  |  |
| _next\_training\_epoch_ | epoch, train\_epochs |  |  |
| _after\_training_ | ‚úñ |  |  |

**Results Namespace**: `step_id`, `epoch`, `train_dataset`, `train_data_loader` \(available as they are published/updated in the train namespace\)

For the **Testing Flow**:

| Methods | Arguments Namespace |
| :--- | :--- |
| _before\_testing_ | ‚úñ |
| _set\_initial\_test\_step\_id_ | step\_info, dataset\_part |
| _before\_step\_testing_ | ‚úñ |
| _set\_task\_layer_ | model, classifier\_field, step\_info, step\_id |
| _make\_test\_dataset_ | step\_info, step\_id |
| _adapt\_test\_dataset_ | ‚úñ |
| _make\_test\_dataloader_ | test\_dataset, num\_workers, test\_mb\_size |
| _before\_testing\_epoch_ | ‚úñ |
| _testing\_epoch_ | ‚úñ |
| _after\_step\_testing_ | ‚úñ |
| _next\_testing\_step_ | step\_id, step\_info, test\_part |
| _after\_testing_ | ‚úñ |

**Results Namespace**: `test_dataset`, `test_data_loader`, `step_id` \(available as they are published/updated in the test namespace\)

Finally, these are the **Self Namespace** parameters always accessible to both flows: `classifier_field`, `model`, `task_layers`, `device`, `test_mb_size`, `train_epochs`, `train_mb_size`

### Advanced Mode

If the standard training and testing flows, for some reason, do not fit in your ideal continual learning strategy, you're free to change them as well. You can add new callbacks \(inject a new method into the existing flow\) or even change the order of the existing one. Everthing is possibile in _Avalanche_! :-\)

In this case we suggest to start by taking a look at the classes contained in the `skeletons` and `templates` subpackage in the API doc:

* `StrategyFlow` and `FlowGroup`
* `IStrategy`, `StrategySkeleton` and `StrategyTemplate`
* `DeepLearningStrategy` and `MTDeepLearningStrategy`

Othrwise, you can just ask us to change the current training and testing flows opening a new issue, and we will make sure to help you!

## üîå Plugins

As we previously hinted **Plugins** are modules implementing _some specific behaviours you can use to build your strategy more easily_.

Under the hoods, _plugins_ operates similarly to strategies \(they specify some methods called by the flow\). However, they cannot be run standalone, do not respect the `DeepLearningStrategy` API and do not necessarily implement the _train_ and _test_ methods.

The basic idea is rather to **attach them to the main strategy so that they can augment its behaviours.** In practice this means that in the flow execution, for every method of the main strategy, the pugin\(s\) corresponding methods **will be called first** \(if implemented\).

An example of Plugin available in _Avalanche_ is the **Evaluation Plugin**. This plugin, upon proper update of the namespace, will handle automatically, all the call to the evaluation protocol. A plugin can be added to the main strategy simply as writing in the constructor:

```python
from avalanche.training.plugins import EvaluationPlugin
self.add_module(EvaluationPlugin())
```

or they can be passed as parameters at the moment of the Strategy instantiation \(please keep in mind that the Evaluation Plugin has been already added for you in the `MTDeepLearningStrategy`\).

At the moment, in _Avalanche_ are available the following plugins:

```python
from avalanche.training.plugins import EvaluationPlugin, ReplayPlugin
```

### Create your Plugin

Creating a plugin is rather straightforward in _Avalanche_. You simply need to create a class inheriting from `StrategySkeleton` and implement the callbacks of your choice.

This is, for example, the implementation of the replay plugin:

```python
from avalanche.training.skeletons import StrategySkeleton
from avalanche.training.skeletons import TrainingFlow

class ReplayPlugin(StrategySkeleton):
    """
    An experience replay plugin that can be plugged in a strategy.

    Instances of this class should be used as strategy plugins.

    This simply handles an external memory filled with randomly selected
    patterns and implements the "adapt_train_dataset" callback to add them to
    the training set.

    The :mem_size: params controls the number of patterns to be stored in the
    external memory. We assume the training set to contain at least this
    number of training data points.
    """
    def __init__(self, mem_size=200):

        super().__init__()

        self.mem_size = mem_size
        self.ext_mem = None

    @TrainingFlow
    def adapt_train_dataset(self, step_id, train_dataset, ext_mem):
        """ Before training we make sure to publish in the namespace a copy
            of :mem_size: randomly selected patterns to be used for replay
            in the next batch and we expand the current training set to
            contain also the data from the external memory. """

        # Additional set of the current batch to be concatenated to the ext.
        # memory at the end of the training
        rm_add = None

        # how many patterns to save for next iter
        h = min(self.mem_size // (step_id + 1), len(train_dataset))

        # We recover it as a mini-batch from the shuffled dataset
        # and we publish it in the namespace
        data_loader = DataLoader(
            train_dataset, batch_size=h, shuffle=True
        )
        rm_add = next(iter(data_loader))
        rm_add = TensorDataset(rm_add[0], rm_add[1])
        self.update_namespace(rm_add=rm_add)

        if step_id > 0:
            # We update the train_dataset concatenating the external memory.
            # We assume the user will shuffle the data when creating the data
            # loader.
            train_dataset = ConcatDataset([train_dataset, ext_mem])
            self.update_namespace(train_dataset=train_dataset)

    @TrainingFlow
    def after_training(self, step_id, rm_add):
        """ After training we update the external memory with the patterns of
         the current training batch/task. """

        # replace patterns in random memory
        ext_mem = self.ext_mem
        if step_id == 0:
            ext_mem = copy.deepcopy(rm_add)
        else:
            idxs_2_replace = torch.randperm(
                len(ext_mem.tensors[0]))[:len(rm_add.tensors[0])]
            for j, idx in enumerate(idxs_2_replace):
                ext_mem.tensors[0][idx] = rm_add.tensors[0][j]
                ext_mem.tensors[1][idx] = rm_add.tensors[1][j]
        self.ext_mem = ext_mem


__all__ = ['ReplayPlugin']
```

In general we recommed to implement a Strategy via pugins, if possible. This will allow other people to use them and facilitate interoperability among different approaches.

For example, we could have implemented a replay strategy as a standalone `MTDeepLearningStrategy` object. However, creating a plugin to be used in conjuncion with the `Naive` strategy is much better, since _experience replay_ is part of many different continual learning strategies:

```python
from avalanche.training.strategies import Naive

cl_strategy = Naive(
    model, 'classifier', SGD(model.parameters(), lr=0.001, momentum=0.9),
    CrossEntropyLoss(), train_mb_size=100, train_epochs=4, test_mb_size=100,
    plugins=[ReplayPlugin(mem_size=10000)]
)
```

This completes the "_Training_" chapter for the "_From Zero to Hero_" series. We hope you enjoyed it!

## ü§ù Run it on Google Colab

You can run _this chapter_ and play with it on Google Colaboratory:

{% embed url="https://colab.research.google.com/drive/1vjLrdYEHWGH9Rz0cQZzb63BO2yCAsUIT\#scrollTo=ADOrYmNXak23" caption="" %}

