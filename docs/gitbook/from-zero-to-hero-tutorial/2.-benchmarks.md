---
description: Create your Continual Learning Benchmark and Start Prototyping
---

# Benchmarks

Welcome to the "_benchmarks_" tutorial of the "_From Zero to Hero_" series. In this part we will present the functionalities offered by the `Benchmarks` module.

## üéØ Nomenclature and Definitions

First off, let's clarify a bit the nomenclature we are goin to use, introducing the following terms: `Datasets`, `Scenarios`, `Benchmarks` and `Generators`.

* By `Dataset` we mean a **collection of examples** that can be used for training or testing purposes but not already organized to be processed as a stream of batches or tasks. Since Avalanche is based on Pytorch, our Datasets are [torch.utils.Datasets](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset) objects.
* By `Scenario` we mean a **particular setting**, i.e. specificities about the continual stream of data, a continual learning algorithm will face.
* By `Benchmark` we mean a well-defined and carefully tought **combination of a scenario with one or multiple datasets** that we can use to asses our continual learning algorithms.
* By `Generator` we mean a function that **given a specific scenario and a dataset can generate a Benchmark**.

We need also to clarify the possible scenarios we will be considering, following the formalization defined in the semilal work ["Continual Learning for Robotics: Definition, Framework, Learning Strategies, Opportunities and Challenges"](https://arxiv.org/abs/1907.00182):

* **Single-Incremental-Task Scenario \(SIT\)**: In this scenario we have just a _single task_, hence the continual stream of data is defined in batches rather than tasks.
* **Multi-Task Scenario \(MT\)**: In this scenario we encounter a sequence of tasks. Of course only encontering examples of new classes makes sense here.
* **Multiple-Incremental-Task Scenario \(MIT\)**: In this scenario a sequence of tasks is encountered over time. However, the same task may appear more than once in the sequence. This is the most realistic yet unexplored setting in litterature. Encountering examples of new as well as old classes is possible here.

These scenario can be even better specialized based on the _update content type_ of their incremental batches/tasks. In particular for each batch/task we may encounter:

* **New Instances \(NI\)**: new instances \(examples\) of previously encountered classes.
* **New Classes \(NC\)**: new instances \(examples\) of new classes, never encountered before.
* **New Instances and Classes \(NIC\)**: new instances \(examples\) of both previously encontered classes as well as new ones.

Below a table is reported to summarize the possible scenario specializations. Having such an apparently overcomplex categorization will help us characterize the behaviors of our benchmarks utltimately offering better flexibility and support to the final user.

|  | New Instances \(NI\) | New Classes \(NC\) | New Instrances and Classes \(NIC\) |
| :---: | :---: | :---: | :---: |
| SIT | ‚úì | ‚úì | ‚úì |
| MT | ‚úñ | ‚úì | ‚úñ |
| MIT | ‚úì | ‚úì | ‚úì |

In _Avalanche_ at the moment we support **all the scenarios** but with a paricular focus on the most used ones in litterature: SIT-NI, SIT-NC, MT-NC.

## üìö The Benchamarks Module

The `bechmarks` module offers 3 types of utils:

* **Datasets**: all the Pytorch datasets plus additional ones prepared by our community and particularly interesting for continual learning.  
* **Classic Benchmarks**: classic benchmarks used in CL litterature ready to be used with great flexibility.
* **Benchmarks Generators**: a set of functions you can use to create your own benchmark starting from any kind of data and scenario. In particular, we distinguish two type of generators: `Specific` and `Generic`. The first ones will let you create a benchmark based on a clear scenario definition commonly used in litterature and Pytorch dataset\(s\); the latters, instead, are more generic and flexible, both in terms of scenario definition then in terms of type of data they can manage. 
  * _Specific_:
    * **NCScenario**: Given one or multiple datasets it creates benchmark based on the MT-NC or SIT-NC scenario.
    * **NIScenario**: Given one or multiple datasets it creates benchmark based on the SIT-NI scenario.
    * **NICScenario**: _yet to implement_.
    * **TransformationScenario**: _yet to implement_.
  * _Generic_:
    * **FilelistScenario**: It creates a benchmark given a set of filelists and based on a generic scenario for maximal flexibility.
    * **TensorScenario**: It creates a benchmark given a set of tensors and based on a generic scenario for maximal flexibility.
    * **DatasetScenario**: It creates a benchmark given a set of pytorch datasets and based on a generic scenario for maximal flexibility.

But let's see how we can use this module in practice!

## üñºÔ∏è Datasets

Let's start with the `Datasets`. As we prevously hinted, in _Avalanche_ you'll find all the standard Pytorch Datasets available in the torchvision package as well as a few others that are useful for continual learning but not already officially available within the Pytorch ecosystem.

```python
from avalanche.benchmarks.datasets import MNIST, FashionMNIST, KMNIST, EMNIST, \
QMNIST, FakeData, CocoCaptions, CocoDetection, LSUN, ImageNet, CIFAR10, \
CIFAR100, STL10, SVHN, PhotoTour, SBU, Flickr8k, Flickr30k, VOCDetection, \
VOCSegmentation, Cityscapes, SBDataset, USPS, Kinetics400, HMDB51, UCF101, \
CelebA, CORe50, TinyImagenet

# As we would simply do with any pytorch dataset we can create the train and 
# test sets from it. We could use any of the above imported Datasets, but let's
# just try to use the standard MNIST.
train_MNIST = MNIST(
    ".", train=True, download=True, transform=torchvision.transforms.ToTensor()
)
test_MNIST = MNIST(
    ".", train=False, download=True, transform=torchvision.transforms.ToTensor()
)

# Given these two sets we can simply iterate them to get the examples one by one
for i, example in enumerate(train_MNIST):
    pass
print("Num. examples processed: {}".format(i))

# or use a Pytorch DataLoader
train_loader = torch.utils.data.DataLoader(
    train_MNIST, batch_size=32, shuffle=True
)
for i, (x, y) in enumerate(train_loader):
    pass
print("Num. mini-batch processed: {}".format(i))
```

Of course also the basic utilities `ImageFolder` and `DatasetFolder` can be used. These are two classes that you can use to create a Pytorch Dataset directly from your files \(following a particular structure\). You can read more about these in the Pytorch official documentation [here](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder).

```python
from avalanche.benchmarks.datasets import ImageFolder, DatasetFolder
```

## üèõÔ∏è Classic Benchmarks

In this section we are going to explore common benchmarks already available for you with one line of code yet flexible anough to allow proper tuning based on your needs.

```python
from avalanche.benchmarks.classic import CORe50, SplitTinyImageNet, \
SplitCIFAR10, SplitCIFAR100, SplitCIFAR110, SplitMNIST, RotatedMNIST, PermutedMNIST

# creating PermutedMNIST with
clscenario = PermutedMNIST(
    incremental_steps=2,
    seed=1234,
)
```

### How to Use them: an MNIST Example

Let's see now how we can use our `PermutedMNIST` benchmark. First let's remember that `PermutedMNIST` follows a _Multi-Task NC_ settings, so for each step we encounter a different task with semantically different classes.

In _Avalanche_ each Benchmark is an instance of the class `GenericCLScenario` or its sub-classes:

* `NCMultiTaskScenario`
* `NCSingleTaskScenario`
* `NISingleTaskScenario`

`perm_mnist` in the example below, is an instance of the object `NCMultiTaskScenario` which essentially defines its attributes and methods specialization.

The nice thing about the _Avalanche_ benchmark is that you can simply iterate through them to get the train and test sets at each step, other them a lot of useful information!

```python
perm_mnist = PermutedMNIST(
    incremental_steps=3,
    seed=1234,
)

for step in perm_mnist:
    print("Start of task ", step.current_task)
    print('Classes in this task:', step.classes_in_this_task)

    # Here's what you can do with the NIStepInfo object
    current_training_set = step.current_training_set()
    training_dataset, t = current_training_set
    print('Task {} batch {} -> train'.format(t, step.current_task))
    print('This task contains', len(training_dataset), 'patterns')

    complete_test_set = step.complete_test_sets()
```

### GenericStep API: What you can do with it

So, as we have seen, each Scenario object in avalanche can be used as an iterator that produces a step, containing all the useful data about that particular batch or task.

Each step is an instance of the `GenericStepInfo` class or its subclasses:

* `NCBatchInfo`
* `NIBatchInfo`
* `NCTaskInfo`

this class offers a comprehensive API for getting any data you may want from the past and the future as well! This is very useful, especially for evaluating your system at any point in time during training.

```python
# Each method returns a list of tuple (dataset, t_label)
# or a single tuple (dataset, t_label) as for the step_specific_test_set
# and step_specific_training_set methods.

# for training
list_of_tuple = step.future_training_sets()
list_of_tuple = step.complete_training_sets()
list_of_tuple = step.cumulative_training_sets()
dataset, t_label = step.step_specific_training_set(step_id=0)

#for testing
list_of_tuple = step.cumulative_test_sets()
list_of_tuple = step.current_test_set()
list_of_tuple = step.future_test_sets()
dataset, t_label = step.step_specific_test_set(step_id=0)

# As always, we can iterate over it normally or with a pytorch
# data loader.
for i, data in enumerate(dataset):
    pass
print("Number of examples: ", i+1)
print("Task Label:", t_label)
```

## üê£ Benchmarks Generators

What if we want to create a new benchmark that is not present in the "_Classic_" ones? Well, in that case _Avalanche_ offer a number of utilites that you can use to create your own benchmark with maximum flexibilty!

### Specific Generators

The _specific_ scenario generators are useful when starting from one or multiple pytorch datasets you want to create a "**New Instances**" or "**New Classes**" benchmark: i.e. it supports the easy and flexible creation of a SIT-NI, SIT-NC or MT-NC scenarios.

For the **New Classes** scenario you can use the following function:

* `NCScenario`

for the **New Instances**:

* `NIScenario`

```python
from avalanche.benchmarks.generators import NIScenario, NCScenario
```

Let's start by creating the MNIST dataset object as we would normally do in Pytorch:

```python
train_transform = Compose([
    RandomCrop(28, padding=4),
    ToTensor(),
    Normalize((0.1307,), (0.3081,))
])

test_transform = Compose([
    ToTensor(),
    Normalize((0.1307,), (0.3081,))
])

mnist_train = MNIST(
    './data/mnist', train=True, download=True, transform=train_transform
)
mnist_test = MNIST(
    './data/mnist', train=False, download=True, transform=test_transform
)
```

Then we can, for example, create a new benchmark based on MNIST and the classic SIT-NI scenario:

```python
ni_scenario = NIScenario(
    mnist_train, mnist_test, n_batches=10, shuffle=True, seed=1234,
    balance_batches=True
)

for step in ni_scenario:
    training_dataset, t = step.current_training_set()
    print('Task {} batch {} -> train'.format(t, step.current_batch))
    print('This batch contains', len(training_dataset), 'patterns')
```

Or, we can create a benchamark based on MNIST and the SIT-NC \(what's commonly referred to as "_Split-MNIST_" benchmark\):

```python
nc_scenario = NCScenario(
    mnist_train, mnist_test, n_steps=10, shuffle=True, seed=1234,
    multi_task=False
)

for step in nc_scenario:
    training_dataset, t = step.current_training_set()
    print('Task {} batch {} -> train'.format(t, step.current_batch))
    print('This batch contains', len(training_dataset), 'patterns')
```

### Generic Generators

Finally, if you cannot create your ideal benchmark since it does not fit well in the aforementioned SIT-NI, SIT-NC or MT-NC scenarios, you can always use our **generic generators**:

* **FilelistScenario**
* **DatasetScenario**
* **TensorScenario**

```python
from avalanche.benchmarks.generators import FilelistScenario, DatasetScenario, \
                                            TensorScenario
```

Let's start with the `FilelistScenario` utility. This function is particularly useful when it is important to preserve a particular order of the patterns to be processed \(for example if they are frames of a video\), or in general if we have data scattered around our drive and we want to create a sequence of batches/tasks providing only a txt containing the list of their paths.

For _Avalanche_ we follow the same format of the _Caffe_ filelists \("_path_ _class\_label_"\):

/path/to/a/file.jpg 0  
/path/to/another/file.jpg 0  
...  
/path/to/another/file.jpg M  
/path/to/another/file.jpg M  
...  
/path/to/another/file.jpg N  
/path/to/another/file.jpg N

So let's download the classic "_Cats vs Dogs_" dataset as an example:

```python
wget --no-check-certificate \
    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip 
unzip cats_and_dogs_filtered.zip
```

You can now see in the `content` directory on colab the image we downloaded. We are now going to create the filelists and then use the `FilelistScenario` function to create our benchamrk:

```python
# let's create the filelists since we don't have it
dirpath = "cats_and_dogs_filtered/train"

for filelist, dir, t_label in zip(
        ["train_filelist_00.txt", "train_filelist_01.txt"],
        ["cats", "dogs"],
        [0, 1]
    ):
    filenames_list = os.listdir(os.path.join(dirpath, dir))
    with open(filelist, "w") as wf:
        for name in filenames_list:
            wf.write(
                "{} {}\n".format(os.path.join(dirpath, dir, name), t_label)
            )

# Here we create a GenericCLScenario ready to be iterated
generic_scenario = FilelistScenario(
   "",
   ["train_filelist_00.txt", "train_filelist_01.txt"],
   "train_filelist_00.txt",
   [0, 0],
   complete_test_set_only=True,
   train_transform=ToTensor(),
   test_transform=ToTensor()
)
```

Let us know see how we can use the `DatasetScenario` utility, where we can use several pytorch datasets as different batches or tasks:

```python
train_cifar10 = CIFAR10(
    './data/mnist', train=True, download=True
)
test_cifar10 = CIFAR10(
    './data/mnist', train=False, download=True
)
generic_scenario =  DatasetScenario(
    train_dataset_list=[train_MNIST, train_cifar10],
    test_dataset_list=[test_MNIST, test_cifar10],
    task_labels=[0, 1]
)
```

And finally, the `TensorScenario` generator:

```python
generic_scenario = TensorScenario(
    train_data_x=[torch.zeros(2,3), torch.ones(2,3)],    
    train_data_y=[torch.zeros(2), torch.ones(2,3)],
    test_data_x=[torch.zeros(2,3)],
    test_data_y=[torch.zeros(2,3)],
    task_labels=[0, 0],
    complete_test_set_only=True
)
```

This completes the "_Benchmark_" tutorial for the "_From Zero to Hero_" series. We hope you enjoyed it!

## ü§ù Run it on Google Colab

You can run _this chapter_ and play with it on Google Colaboratory:

{% embed url="https://colab.research.google.com/drive/1gVQPCSIwLGyRCmJbO3fDQJe7cKxgsBfd?usp=sharing" caption="" %}

