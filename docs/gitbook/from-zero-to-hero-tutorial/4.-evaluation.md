---
description: Automatic Evaluation with Pre-implemented Protocols and Metrics
---

# Evaluation

{% hint style="danger" %}
This doc is out-of-date. Check the [corresponding notebook](https://colab.research.google.com/drive/1SJPfjqaZh5QucV8MioMM3GcYyR2Krurj?usp=sharing) for the latest version.
{% endhint %}

Welcome to the "_Evaluation_" tutorial of the "_From Zero to Hero_" series. In this part we will present the functionalities offered by the `evaluation` module.

## üìà The Evaluation Module

The `evaluation` module is quite straightforward at the moment and offers 3 main submodules:

* **Metrics**: a set of classes \(one for metric\) which implement the main continual learning matrics computation like _accuracy_, _forgetting_, _memory usage_, _running times_, etc.
* **TensorboardLogging**: offers a main class to handles Tensorboard logging configuration with nice plots updated on-the-fly to control and "_babysit_" your experiment easily.
* **Evaluation Protocols**: this module provides a single point of entry to the evaluation methodology configuration and in charge of hangling the metrics computation, tensorboard or avanced console logging.

## üìä Metrics

In _Avalanche_ we offer at the moment a number of pre-implemented metrics you can use for your own experiments. We made sure to include all the major accuracy-based matrics but also the ones related to computation and memory.

The metrics already available \(soon to be expanded\) are:

* **Accuracy** \(`ACC`\): Accuracy over time \(Total average or per task\).
* **Catastrophic Forgetting** \(`CF`\): Forgetting as defined in \(Lopez-paz 2017\).
* **RAM Usage** \(`RAMU`\): RAM usage by the process over time.
* **Confusion Matrix** \(`CM`\): Confusion matrix over time.
* **CPU Usage** \(`CPUUsage`\): CPU usage by the process over time.
* **GPU Usage** \(`GPUUsage`\): GPU usage by the process over time.
* **Disk Usage** \(`DiskUsage`\): Disk usage by the process over time.
* **Time Usage** \(`TimeUsage`\): Running time of the python process.

```python
from avalanche.evaluation.metrics import ACC, CF, RAMU, CM, CPUUsage, GPUUsage,\
 DiskUsage, TimeUsage
```

While each metric can be directly managed within and _Evaluation Protocol_ \(see next section\), we can use each metric directly, being them simply python classes. For example the accuracy metric works as follows:

```python
real_y = np.asarray([1, 2])
predicted_y = np.asarray([1, 0])
acc_metric = ACC()
acc, acc_x_class = acc_metric.compute([real_y], [predicted_y])

print("Average Accuracy:", acc)
print("Accuracy per class:", acc_x_class)
```

## üìô Tensorboard

Tensorboard has consolidated its position as the go-to **visualization toolkit** for deep learning experiments for both pytorch and tensorflow. In _Avalanche_ we decided the different the different metrics \(and their eventual change over time\) using the standard Pytorch version.

At the moment we implemented just the `TensorboardLogging` object that can be used to specify Tensorboard behavious with eventual customizations.

```python
from avalanche.evaluation.tensorboard import TensorboardLogging

# a simple example of tensorboard instantiation
tb_logging = TensorboardLogging(tb_logdir=".")
```

## üìêEvaluation Protocol

The **Evaluation Protocol**, is the object in charge of configuring and controlling the evaluation procedure. This object can be passed to a Strategy that will automatically call its main functionalities when the **training and testing flows** are activated.

```python
from avalanche.evaluation.metrics import ACC, CF, RAMU, CM
from avalanche.extras.models import SimpleMLP
from avalanche.training.strategies import Naive
from avalanche.evaluation import EvalProtocol

# load the model with PyTorch for example
model = SimpleMLP()

# Eval Protocol
evalp = EvalProtocol(
    metrics=[ACC(), CF(), RAMU(), CM()], tb_logdir='.'
)

# adding the CL strategy
clmodel = Naive(model, eval_protocol=evalp)
```

This completes the "_Evaluation_" tutorial for the "_From Zero to Hero_" series. We hope you enjoyed it!

## ü§ù Run it on Google Colab

You can run _this chapter_ and play with it on Google Colaboratory:

{% embed url="https://colab.research.google.com/drive/1SJPfjqaZh5QucV8MioMM3GcYyR2Krurj?usp=sharing" %}



