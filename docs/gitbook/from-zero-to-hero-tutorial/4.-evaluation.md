---
description: Automatic Evaluation with Pre-implemented Metrics
---

# Evaluation

Welcome to the "_Evaluation_" tutorial of the "_From Zero to Hero_" series. In this part we will present the functionalities offered by the `evaluation` module.

## üìà The Evaluation Module

The `evaluation` module is quite straightforward at the moment as it offers all the basic functionalities to evaluate keep track of a continual learning experiment. This is mostly done throught the **Metrics**: a set of classes \(one for metric\) which implement the main continual learning matrics computation like _accuracy_, _forgetting_, _memory usage_, _running times_, etc.

The metrics will be in turn managed by the `EvaluationPlugin`, a standard _Avalanche_ plugin \(as discussed in the previous tutorial\) but in charge of handling the evaluation part of an experiment. The `EvaluationPlugin` will indeed not only call these metrics at the appropriate time, but also be also in charge of collecting and logging appropriately their results.

## üìä Metrics

In _Avalanche_ we offer at the moment a number of pre-implemented metrics you can use for your own experiments. We made sure to include all the major accuracy-based matrics but also the ones related to computation and memory.

The metrics already available in the current _Avalanche_ release are:

```python
from avalanche.evaluation.metrics import Accuracy, MinibatchAccuracy, \
EpochAccuracy, RunningEpochAccuracy, ExperienceAccuracy, ConfusionMatrix, \
StreamConfusionMatrix, CPUUsage, MinibatchCPUUsage, EpochCPUUsage, \
RunningEpochCPUUsage, ExperienceCPUUsage, DiskUsage, disk_usage_metrics, \
ExperienceForgetting, MaxGPU, Loss, MinibatchLoss, \
EpochLoss, RunningEpochLoss, ExperienceLoss, MAC, Mean, MaxRAM, \
Sum, ElapsedTime, MinibatchTime, EpochTime, RunningEpochTime, ExperienceTime, \
timing_metrics
```

_Avalanche_ offers also a set of "_metrics generators_" that can help you create a set of metrics that may used in your experiments:

```python
from avalanche.evaluation.metrics import accuracy_metrics, cpu_usage_metrics, \
loss_metrics
```

While each metric can be directly managed within and `EvaluationPlugin` \(see next section\), we can use each metric directly \(similarly to [tf.keras.metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)\), being them simply python classes. For example the accuracy metric works as follows:

```python
real_y = torch.tensor([1, 2])
predicted_y = torch.tensor([1, 0])
acc_metric = Accuracy()
acc_metric.update(real_y, predicted_y)

print("Average Accuracy:", acc_metric.result())
```

## üìêEvaluation Plugin

The **Evaluation Plugin**, is the object in charge of configuring and controlling the evaluation procedure. This object can be passed to a Strategy as as a "special" plugin through the evaluator attribute.

```python
from avalanche.benchmarks.classic import SplitMNIST
from avalanche.evaluation.metrics import ExperienceForgetting, accuracy_metrics,
loss_metrics, timing_metrics, cpu_usage_metrics, StreamConfusionMatrix,
disk_usage_metrics, gpu_usage_metrics
from avalanche.models import SimpleMLP
from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger
from avalanche.training.plugins import EvaluationPlugin
from avalanche.training.strategies import Naive

scenario = SplitMNIST(n_experiences=5)

# MODEL CREATION
model = SimpleMLP(num_classes=scenario.n_classes)

# DEFINE THE EVALUATION PLUGIN 
# The evaluation plugin manages the metrics computation.
# It takes as argument a list of metrics, collectes their results and returns 
# them to the strategy it is attached to.

eval_plugin = EvaluationPlugin(
    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),
    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),
    timing_metrics(epoch=True, epoch_running=True),
    cpu_usage_metrics(experience=True),
    ExperienceForgetting(),
    StreamConfusionMatrix(num_classes=scenario.n_classes, save_image=False),
    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True)
)

# CREATE THE STRATEGY INSTANCE (NAIVE)
cl_strategy = Naive(
    model, SGD(model.parameters(), lr=0.001, momentum=0.9),
    CrossEntropyLoss(), train_mb_size=500, train_epochs=1, eval_mb_size=100,
    evaluator=eval_plugin)

# TRAINING LOOP
print('Starting experiment...')
results = []
for experience in scenario.train_stream:
    print("Start of experience: ", experience.current_experience)
    print("Current Classes: ", experience.classes_in_this_experience)

    # train returns a dictionary which contains all the metric values
    res = cl_strategy.train(experience, num_workers=4)
    print('Training completed')

    print('Computing accuracy on the whole test set')
    # test also returns a dictionary which contains all the metric values
    results.append(cl_strategy.eval(scenario.test_stream, num_workers=4))
```

This completes the "_Evaluation_" tutorial for the "_From Zero to Hero_" series. We hope you enjoyed it!

## ü§ù Run it on Google Colab

You can run _this chapter_ and play with it on Google Colaboratory:

{% embed url="https://colab.research.google.com/drive/1SJPfjqaZh5QucV8MioMM3GcYyR2Krurj?usp=sharing" caption="" %}

